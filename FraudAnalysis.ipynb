{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOY4KNkdMSLLgrGns/DzZmA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iraj259/Machine-Learning/blob/main/FraudAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "_sE3UEm1GENf"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when, log1p, isnan, count, sum as spark_sum, expr\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        ".appName(\"FraudDetection\") \\\n",
        ".config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        ".getOrCreate()\n",
        "\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"WARN\")"
      ],
      "metadata": {
        "id": "LlwawwRGGSep"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path = \"./fraudDetection.csv\""
      ],
      "metadata": {
        "id": "aFq2AV1FGZxk"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema = StructType([\n",
        "StructField(\"step\", IntegerType(), True),\n",
        "StructField(\"type\", StringType(), True),\n",
        "StructField(\"amount\", DoubleType(), True),\n",
        "StructField(\"nameOrig\", StringType(), True),\n",
        "StructField(\"oldbalanceOrg\", DoubleType(), True),\n",
        "StructField(\"newbalanceOrig\", DoubleType(), True),\n",
        "StructField(\"nameDest\", StringType(), True),\n",
        "StructField(\"oldbalanceDest\", DoubleType(), True),\n",
        "StructField(\"newbalanceDest\", DoubleType(), True),\n",
        "StructField(\"isFraud\", DoubleType(), True),\n",
        "StructField(\"isFlaggedFraud\", DoubleType(), True)\n",
        "])\n",
        "\n",
        "\n",
        "df = spark.read.csv(csv_path, header=True, schema=schema)\n",
        "\n",
        "\n",
        "print(\"Schema:\")\n",
        "df.printSchema()\n",
        "print(\"Row count:\", df.count())"
      ],
      "metadata": {
        "id": "x_IcjfYHGfUf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96fdcf07-ae69-4187-b34f-dbb941d67fff"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schema:\n",
            "root\n",
            " |-- step: integer (nullable = true)\n",
            " |-- type: string (nullable = true)\n",
            " |-- amount: double (nullable = true)\n",
            " |-- nameOrig: string (nullable = true)\n",
            " |-- oldbalanceOrg: double (nullable = true)\n",
            " |-- newbalanceOrig: double (nullable = true)\n",
            " |-- nameDest: string (nullable = true)\n",
            " |-- oldbalanceDest: double (nullable = true)\n",
            " |-- newbalanceDest: double (nullable = true)\n",
            " |-- isFraud: double (nullable = true)\n",
            " |-- isFlaggedFraud: double (nullable = true)\n",
            "\n",
            "Row count: 6362620\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "null_counts = df.select([spark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
        "null_counts.show(truncate=False)"
      ],
      "metadata": {
        "id": "31TXBz1SGt2s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "965b3010-7231-4d9c-f349-f2da9e7a97ed"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+------+--------+-------------+--------------+--------+--------------+--------------+-------+--------------+\n",
            "|step|type|amount|nameOrig|oldbalanceOrg|newbalanceOrig|nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|\n",
            "+----+----+------+--------+-------------+--------------+--------+--------------+--------------+-------+--------------+\n",
            "|0   |0   |0     |0       |0            |0             |0       |0             |0             |0      |0             |\n",
            "+----+----+------+--------+-------------+--------------+--------+--------------+--------------+-------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_stats = df.select(\"step\",\"amount\",\"oldbalanceOrg\",\"newbalanceOrig\",\"oldbalanceDest\",\"newbalanceDest\",\"isFraud\").describe()\n",
        "numeric_stats.show()"
      ],
      "metadata": {
        "id": "6OO1aTL0G1Kv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a41a84c8-229d-4709-9e90-aa23b355eb5b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+-----------------+-----------------+------------------+------------------+------------------+--------------------+\n",
            "|summary|              step|           amount|    oldbalanceOrg|    newbalanceOrig|    oldbalanceDest|    newbalanceDest|             isFraud|\n",
            "+-------+------------------+-----------------+-----------------+------------------+------------------+------------------+--------------------+\n",
            "|  count|           6362620|          6362620|          6362620|           6362620|           6362620|           6362620|             6362620|\n",
            "|   mean|243.39724563151657|179861.9035491287|833883.1040744764| 855113.6685785812|1100701.6665196533|1224996.3982019224|0.001290820448180152|\n",
            "| stddev|142.33197104913066|603858.2314629209|2888242.673037527|2924048.5029542595|3399180.1129944525|3674128.9421196915|0.035904796801604424|\n",
            "|    min|                 1|              0.0|              0.0|               0.0|               0.0|               0.0|                 0.0|\n",
            "|    max|               743|    9.244551664E7|    5.958504037E7|     4.958504037E7|    3.5601588935E8|    3.5617927892E8|                 1.0|\n",
            "+-------+------------------+-----------------+-----------------+------------------+------------------+------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(\"isFlaggedFraud\", \"nameOrig\", \"nameDest\")\n",
        "df = df.filter(col(\"isFraud\").isNotNull())"
      ],
      "metadata": {
        "id": "uBjiXF1wG6Rw"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropDuplicates()"
      ],
      "metadata": {
        "id": "2qibQ7SOG8Uj"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_dist = df.groupBy(\"isFraud\").count().toPandas()\n",
        "print(class_dist)"
      ],
      "metadata": {
        "id": "4gk69HgYHm-C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af376f42-77c6-4c3d-fd85-cf9556a3160b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   isFraud    count\n",
            "0      0.0  6353880\n",
            "1      1.0     8197\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fraud_by_type = df.groupBy(\"type\").agg(count(\"type\").alias(\"count\"), spark_sum(col(\"isFraud\")).alias(\"fraud_count\"))\n",
        "fraud_by_type = fraud_by_type.withColumn(\"fraud_rate\", col(\"fraud_count\")/col(\"count\"))\n",
        "fraud_by_type.orderBy(col(\"fraud_rate\").desc()).show()"
      ],
      "metadata": {
        "id": "LmUDIJzIHxju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0dfb738-cb68-4b10-d8fe-3d83660b3c59"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------+-----------+--------------------+\n",
            "|    type|  count|fraud_count|          fraud_rate|\n",
            "+--------+-------+-----------+--------------------+\n",
            "|TRANSFER| 532909|     4097.0|0.007687991758442811|\n",
            "|CASH_OUT|2237484|     4100.0|0.001832415337942...|\n",
            "| CASH_IN|1399284|        0.0|                 0.0|\n",
            "| PAYMENT|2150968|        0.0|                 0.0|\n",
            "|   DEBIT|  41432|        0.0|                 0.0|\n",
            "+--------+-------+-----------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "time_fraud = df.groupBy(\"step\").agg(count(\"step\").alias(\"volume\"), spark_sum(col(\"isFraud\")).alias(\"fraud_count\"))\n",
        "time_fraud = time_fraud.withColumn(\"fraud_rate\", col(\"fraud_count\")/col(\"volume\"))\n",
        "time_fraud.orderBy(\"step\").show(24)"
      ],
      "metadata": {
        "id": "KBEsCARrIem4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import (round as spark_round)"
      ],
      "metadata": {
        "id": "1aK5VOOUIgSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn(\"orig_balance_diff\", col(\"oldbalanceOrg\") - col(\"newbalanceOrig\")) \\\n",
        ".withColumn(\"dest_balance_diff\", col(\"newbalanceDest\") - col(\"oldbalanceDest\")) \\\n",
        ".withColumn(\"orig_balance_zero\", when((col(\"oldbalanceOrg\") > 0) & (col(\"orig_balance_diff\") == 0), 1.0).otherwise(0.0)) \\\n",
        ".withColumn(\"dest_balance_zero\", when((col(\"amount\") > 0) & (col(\"dest_balance_diff\") == 0), 1.0).otherwise(0.0))"
      ],
      "metadata": {
        "id": "q34D6qegIwol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for c in [\"amount\",\"oldbalanceOrg\",\"newbalanceOrig\",\"oldbalanceDest\",\"newbalanceDest\"]:\n",
        "     df = df.withColumn(f\"log_{c}\", log1p(col(c)))"
      ],
      "metadata": {
        "id": "ihUJqqDxIyoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.cache()\n",
        "print('Cached dataframe')"
      ],
      "metadata": {
        "id": "t5FWPsffI35j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
        "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
      ],
      "metadata": {
        "id": "0FcDMoXJI51C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type_indexer = StringIndexer(inputCol=\"type\", outputCol=\"type_index\", handleInvalid='keep')\n",
        "type_encoder = OneHotEncoder(inputCols=[\"type_index\"], outputCols=[\"type_vec\"])"
      ],
      "metadata": {
        "id": "ClsbYcDyI7lA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cols = [\n",
        "'step',\n",
        "'log_amount', 'log_oldbalanceOrg', 'log_newbalanceOrig', 'log_oldbalanceDest', 'log_newbalanceDest',\n",
        "'orig_balance_diff','dest_balance_diff','orig_balance_zero','dest_balance_zero','type_vec'\n",
        "]"
      ],
      "metadata": {
        "id": "BDGQXgNqJASI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assembler = VectorAssembler(\n",
        "inputCols=[c for c in ['step','log_amount','log_oldbalanceOrg','log_newbalanceOrig','log_oldbalanceDest','log_newbalanceDest','orig_balance_diff','dest_balance_diff','orig_balance_zero','dest_balance_zero','type_vec']],\n",
        "outputCol='raw_features',\n",
        "handleInvalid='keep'\n",
        ")\n",
        "\n",
        "\n",
        "scaler = StandardScaler(inputCol='raw_features', outputCol='features')"
      ],
      "metadata": {
        "id": "4b8ihuLlJK_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
        "print('Train count:', train_df.count(), 'Test count:', test_df.count())\n",
        "\n",
        "\n",
        "# If class imbalance is extreme, consider resampling only on training set\n",
        "fraud_count = train_df.filter(col('isFraud') == 1.0).count()\n",
        "nonfraud_count = train_df.filter(col('isFraud') == 0.0).count()\n",
        "print('Train fraud:', fraud_count, 'Train non-fraud:', nonfraud_count)"
      ],
      "metadata": {
        "id": "QTH1DmAPJNhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if fraud_count > 0 and fraud_count < nonfraud_count:\n",
        "  ratio = int(nonfraud_count / fraud_count)\n",
        "fraud_df = train_df.filter(col('isFraud') == 1.0)\n",
        "replicated = fraud_df\n",
        "for i in range(ratio-1):\n",
        "    replicated = replicated.union(fraud_df)\n",
        "train_df = train_df.filter(col('isFraud') == 0.0).union(replicated)\n",
        "print('After replication train counts -> fraud:', train_df.filter(col('isFraud')==1.0).count(), 'nonfraud:', train_df.filter(col('isFraud')==0.0).count())"
      ],
      "metadata": {
        "id": "qMrKn3KAJPxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessing_stages = [type_indexer, type_encoder, assembler, scaler]\n",
        "\n",
        "\n",
        "# Logistic Regression\n",
        "lr = LogisticRegression(featuresCol='features', labelCol='isFraud', maxIter=20)\n",
        "pipeline_lr = Pipeline(stages=preprocessing_stages + [lr])\n",
        "\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestClassifier(featuresCol='features', labelCol='isFraud', numTrees=100)\n",
        "pipeline_rf = Pipeline(stages=preprocessing_stages + [rf])\n",
        "\n",
        "\n",
        "# Gradient-Boosted Trees\n",
        "gbt = GBTClassifier(featuresCol='features', labelCol='isFraud', maxIter=50)\n",
        "pipeline_gbt = Pipeline(stages=preprocessing_stages + [gbt])"
      ],
      "metadata": {
        "id": "aBHhvYQGJWSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = BinaryClassificationEvaluator(labelCol='isFraud', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
        "\n",
        "\n",
        "models = {}\n",
        "metrics = {}\n",
        "\n",
        "\n",
        "for name, pipeline in [('LR', pipeline_lr), ('RF', pipeline_rf), ('GBT', pipeline_gbt)]:\n",
        "    print(f\"Training {name} ...\")\n",
        "t0 = time.time()\n",
        "model = pipeline.fit(train_df)\n",
        "t1 = time.time()\n",
        "print(f\"{name} training time: {t1-t0:.2f}s\")\n",
        "models[name] = model\n",
        "\n",
        "\n",
        "# Predictions\n",
        "preds = model.transform(test_df)\n",
        "auc = evaluator.evaluate(preds)"
      ],
      "metadata": {
        "id": "oYefy24GJYtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tp = preds.filter((col('isFraud')==1.0) & (col('prediction')==1.0)).count()\n",
        "tn = preds.filter((col('isFraud')==0.0) & (col('prediction')==0.0)).count()\n",
        "fp = preds.filter((col('isFraud')==0.0) & (col('prediction')==1.0)).count()\n",
        "fn = preds.filter((col('isFraud')==1.0) & (col('prediction')==0.0)).count()\n",
        "\n",
        "\n",
        "precision = tp / (tp+fp) if (tp+fp) > 0 else 0.0\n",
        "recall = tp / (tp+fn) if (tp+fn) > 0 else 0.0\n",
        "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "\n",
        "metrics[name] = {\n",
        "'auc': auc,\n",
        "'precision': precision,\n",
        "'recall': recall,\n",
        "'f1': f1,\n",
        "'tp': tp,\n",
        "'tn': tn,\n",
        "'fp': fp,\n",
        "'fn': fn,\n",
        "'train_time_s': t1-t0\n",
        "}\n",
        "print(f\"{name} metrics -> AUC: {auc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "JvNiqUvRJfmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\n=== Metrics Summary ===')\n",
        "for k,v in metrics.items():\n",
        "    print(k, v)"
      ],
      "metadata": {
        "id": "PzfOgbavJq2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_model = models.get('RF')\n",
        "if rf_model:\n",
        "  rf_stage = [s for s in rf_model.stages if s.__class__.__name__ == 'RandomForestClassificationModel']\n",
        "if rf_stage:\n",
        "  rf_stage = rf_stage[0]\n",
        "importances = rf_stage.featureImportances\n",
        "print('RF feature importances vector:', importances)"
      ],
      "metadata": {
        "id": "WMOlRaW7JujM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dir = './saved_models'\n",
        "for name, model in models.items():\n",
        "    out = f\"{model_dir}/{name}_pipeline\"\n",
        "    try:\n",
        "        model.write().overwrite().save(out)\n",
        "        print(f\"Saved {name} to {out}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: could not save {name} -> {e}\")"
      ],
      "metadata": {
        "id": "rRrFjaBdJytI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sizes = [0.01, 0.05, 0.1, 0.25, 0.5]\n",
        "benchmarks = []\n",
        "for frac in sample_sizes:\n",
        "     sample_train = train_df.sample(withReplacement=False, fraction=frac, seed=42)\n",
        "t0 = time.time()\n",
        "_ = pipeline_rf.fit(sample_train)\n",
        "t1 = time.time()\n",
        "benchmarks.append((frac, t1-t0))\n",
        "print(f\"RF fit time for {frac*100:.1f}% of train: {t1-t0:.2f}s\")"
      ],
      "metadata": {
        "id": "8CoNOF5nKF3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('metrics_summary.json', 'w') as f:\n",
        "      json.dump(metrics, f)\n",
        "\n",
        "\n",
        "bench_df = spark.createDataFrame(benchmarks, schema=['fraction','seconds'])\n",
        "bench_df.coalesce(1).write.mode('overwrite').option('header',True).csv('./benchmarks')"
      ],
      "metadata": {
        "id": "L1N-KSjNKQq-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}